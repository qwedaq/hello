{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI21RESCH11003_Assign1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkUmvYm+Qpd+xhNRaD4xoC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aveen-d/hello/blob/master/AI21RESCH11003_Assign1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tsPXR1KYZxw",
        "outputId": "a3d53ec5-7945-4780-b8b5-8632a2635c92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV1glnjNGxS_"
      },
      "source": [
        "Method for Stratified Split of entire dataset into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQRCKBw3zuHh"
      },
      "source": [
        "def stratified_train_test_split(data):\n",
        "  num_pos = data[:,-1].tolist().count(1)\n",
        "  num_neg = data[:,-1].tolist().count(0)\n",
        "  print(\"Total positive samples: \" +str(num_pos))\n",
        "  print(\"Total negative samples: \"+str(num_neg))\n",
        "  test_set=[]\n",
        "  train_set=[]\n",
        "  p_count=0\n",
        "  n_count=0\n",
        "  for i in range(data.shape[0]):\n",
        "    if data[i,-1] == 1 and p_count <int(num_pos/10):\n",
        "      test_set.append(data[i])\n",
        "      p_count=p_count+1\n",
        "    elif  data[i,-1] == 0 and n_count <int(num_neg/10):\n",
        "      test_set.append(data[i])\n",
        "      n_count=n_count+1\n",
        "    else:\n",
        "      train_set.append(data[i])\n",
        "  test_set=np.array(test_set)\n",
        "  training_set = np.array(train_set)\n",
        "  return training_set, test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du9DqeQ9G5FO"
      },
      "source": [
        "Method for Stratified K fold split of train dataset into 10 folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNXGS_1Jzw6n"
      },
      "source": [
        "def stratified_K_fold(training_set,n_folds):\n",
        "  pos_data=[]\n",
        "  neg_data=[]\n",
        "  for i in range(training_set.shape[0]):\n",
        "    if training_set[i,-1] == 1:\n",
        "      pos_data.append(training_set[i])\n",
        "    else: \n",
        "      neg_data.append(training_set[i])\n",
        "  pos_data=np.array(pos_data)\n",
        "  neg_data=np.array(neg_data)\n",
        "  pos_folds = np.array_split(pos_data,10)\n",
        "  neg_folds = np.array_split(neg_data, 10)\n",
        "  print(\"Pos_data in training set: \"+str(pos_data.shape))\n",
        "  print(\"Neg_data in training set: \"+str(neg_data.shape))\n",
        "  fold_data=[]\n",
        "  for i in range(n_folds):\n",
        "    temp = np.concatenate((pos_folds[i],neg_folds[i]),axis=0)\n",
        "    np.random.shuffle(temp)\n",
        "    fold_data.append(temp)\n",
        "  return fold_data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOCXto9iz3LF"
      },
      "source": [
        "## Univariate Binary Split Decision Tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIEhMPF0_2p4"
      },
      "source": [
        "Define a class node with the shown attributes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKDahI7Nz8rb"
      },
      "source": [
        "class Node:\n",
        "  def __init__(self):\n",
        "    self.left_child=None\n",
        "    self.right_child=None\n",
        "    self.impurity_value=None\n",
        "    self.feature_value=None\n",
        "    self.threshold = None\n",
        "    self.info_gain = None\n",
        "    self.label=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT9i-V6Z_6tF"
      },
      "source": [
        "Calculate Gini Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGYy51M3z_D0"
      },
      "source": [
        "def gini(data):\n",
        "  # count number of positive and negative samples\n",
        "  n_neg = data.tolist().count(0)\n",
        "  n_pos= data.tolist().count(1)\n",
        "  \n",
        "  # calculate probabilities\n",
        "  p_pos = round(n_pos / data.shape[0], 3)\n",
        "  p_neg = round(n_neg / data.shape[0], 3)\n",
        "\n",
        "  if p_pos == 1 or p_neg == 1:\n",
        "    gini_ = 0\n",
        "  else:\n",
        "    gini_ = 1 - (np.square(p_pos)+ np.square(p_neg))\n",
        "  return gini_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeD4u2YQ_9N4"
      },
      "source": [
        "Calculate Entropy value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYG3j9m0BZj"
      },
      "source": [
        "def entropy(data):\n",
        "  # count number of positive and negative samples\n",
        "  n_neg = data.tolist().count(0)\n",
        "  n_pos= data.tolist().count(1)\n",
        "\n",
        "  #calculate probabilities\n",
        "  p_pos = round(n_pos / data.shape[0], 3)\n",
        "  p_neg = round(n_neg / data.shape[0], 3)\n",
        "  \n",
        "  if p_pos == 1 or p_neg == 1:\n",
        "    entropy_ = 0\n",
        "  else:\n",
        "    entropy_ = - ((p_pos*np.log2(p_pos)) + (p_neg*np.log2(p_neg)))\n",
        "  return entropy_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8LCgytC__y4"
      },
      "source": [
        "Calculate Information gain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVB3MSW90EED"
      },
      "source": [
        "def Info_Gain(left_data, right_data, data):\n",
        "  n_left = left_data.shape[0]\n",
        "  n_right = right_data.shape[0]\n",
        "  total_ = data.shape[0]\n",
        "  ### use gini index  \n",
        "  impurity_value = ((n_left/total_) * gini(left_data))+ ((n_right/total_) * gini(right_data))\n",
        "  info_gain = gini(data) - impurity_value\n",
        "\n",
        "  ### use entropy\n",
        "  #impurity_value = ((n_left/total_) * entropy(left_data))+ ((n_right/total_) * entropy(right_data))\n",
        "  #info_gain = entropy(data) - impurity_value\n",
        "\n",
        "  return info_gain, impurity_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpsAk8vRAZU6"
      },
      "source": [
        "Method to find the feature that creates the best split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRz5clhN0Gg-"
      },
      "source": [
        "def node_split(data):\n",
        "  max_feature={'threshold':None,'impurity_value':None, 'feature_index':None, 'info_gain': None,'left_data':None, 'right_data':None} # dictionary to track best feature to split\n",
        "  max_ig = -100000000\n",
        "  for i in range(data.shape[1]-1):    # Loop over all features\n",
        "    unique_values = np.unique(data[:,i])\n",
        "    for j in range(len(unique_values)-2): # Loop over each unique value of each feature\n",
        "      #mid= (unique_values[j])     # threshold as the unique value\n",
        "      #mid= (unique_values[j] + unique_values[j+1])/2     # threshold as the midpoint of unique value and unique value+1 (change the for loop condition to range(len(unique_values)-1))\n",
        "      mid= (unique_values[j] + unique_values[j+1] + unique_values[j+2])/3     # threshold as the average of unique value, unique value+1 and unique value+2 (change the for loop condition to range(len(unique_values)-2))\n",
        "\n",
        "      # split data into left and right accordingly\n",
        "      \n",
        "      temp1=[]\n",
        "      temp2=[]\n",
        "      for row in data:\n",
        "        if row[i]<=mid:\n",
        "          temp1.append(row)\n",
        "        elif row[i]>mid:\n",
        "          temp2.append(row)\n",
        "      left_data = np.array(temp1)\n",
        "      right_data = np.array(temp2)\n",
        "\n",
        "      #calculate Information Gain and check the maximum information gain\n",
        "      if left_data.shape[0] > 0 and right_data.shape[0] > 0:\n",
        "        info_gain, impurity_value = Info_Gain(left_data[:,-1], right_data[:,-1], data[:,-1])\n",
        "        if info_gain > max_ig:\n",
        "          max_ig = info_gain\n",
        "          max_feature['threshold'] = mid\n",
        "          max_feature['impurity_value'] = impurity_value\n",
        "          max_feature['feature_index'] = i\n",
        "          max_feature['info_gain'] = info_gain\n",
        "          max_feature['left_data'] = left_data\n",
        "          max_feature['right_data'] = right_data\n",
        "  \n",
        "  return max_feature['feature_index'], max_feature['left_data'], max_feature['right_data'], max_feature['threshold'], max_feature['impurity_value'], max_feature['info_gain']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjRrQP9MBwn1"
      },
      "source": [
        "Method that creates tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFZTyom40JEr"
      },
      "source": [
        "def tree(data, current_depth):\n",
        "  # Parameters to control overfitting of Decision Tree\n",
        "  max_depth = 5\n",
        "  min_sample = 10\n",
        "\n",
        "  if(current_depth <= max_depth and data.shape[0] >= min_sample):\n",
        "    feature,left_data,right_data, threshold, impurity_value, info_gain = node_split(data) # find the best feature at this node to split\n",
        "    node=Node()\n",
        "    node.feature_value = feature\n",
        "    node.threshold= threshold\n",
        "    node.impurity_value = impurity_value\n",
        "    node.info_gain = info_gain\n",
        "    node.left_child = tree(left_data, current_depth+1)     # Create left node\n",
        "    node.right_child = tree(right_data, current_depth+1)   # Create right node\n",
        "    return node\n",
        "\n",
        "# Treat as Base Case. Assigns a label to leaf node\n",
        "  node=Node()\n",
        "  pos = data[:,-1].tolist().count(1)\n",
        "  neg = data[:,-1].tolist().count(0)\n",
        "  if pos >= neg:\n",
        "    node.label= int(1)\n",
        "  else:\n",
        "    node.label = int(0)\n",
        "  \n",
        "  return node\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTLI_C440KFE",
        "outputId": "40e2c283-917d-4269-feac-1984eaffa886"
      },
      "source": [
        "# FoML Assign 1 Code Skeleton\n",
        "# Please use this outline to implement your decision tree. You can add any code around this.\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Enter You Name Here\n",
        "myname = \"Aveen-Dayal\" # or \"Amar-Akbar-Antony\"\n",
        "\n",
        "# Implement your decision tree below\n",
        "class DecisionTree():\n",
        "    tree = {}\n",
        "\n",
        "    def learn(self, training_dataset):\n",
        "        # implement this function\n",
        "\n",
        "        self.tree = tree(training_dataset,0)\n",
        "        return self.tree\n",
        "\n",
        "    # implement this function\n",
        "    def classify(self, test_instance,tree):\n",
        "        #result = 0 # baseline: always classifies as 0\n",
        "        if tree.label != None:  \n",
        "          result = tree.label\n",
        "          return result\n",
        "        else:\n",
        "          instance_val = test_instance[tree.feature_value]\n",
        "          if instance_val <= tree.threshold:\n",
        "            return self.classify(test_instance, tree.left_child)\n",
        "          else:\n",
        "            return self.classify(test_instance, tree.right_child)\n",
        "\n",
        "def run_decision_tree():\n",
        "\n",
        "    # Load data set\n",
        "    with open(\"/gdrive/MyDrive/foml datasets/wine-dataset.csv\") as f:\n",
        "        next(f, None)\n",
        "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
        "    print (\"Number of records: \"+ str(len(data)))\n",
        "\n",
        "    # Split training/test sets\n",
        "    # You need to modify the following code for cross validation.\n",
        "\n",
        "    data=np.array(data).astype(np.float)\n",
        "    np.random.shuffle(data)\n",
        "    training_set,test_set = stratified_train_test_split(data) # splits the data into 10% test and 90% train.\n",
        "\n",
        "    # Split dataset into K folds\n",
        "    n_folds = 10\n",
        "    fold_data = stratified_K_fold(training_set, n_folds)\n",
        "\n",
        "    Acc_train=[]\n",
        "    Acc_test=[]\n",
        "\n",
        "    for k in range(n_folds):\n",
        "      test_dataset = fold_data[k]\n",
        "      temp=[]\n",
        "      for j in range(n_folds):\n",
        "        if j != k:\n",
        "          temp.append(fold_data[j])\n",
        "      train_dataset = np.vstack(temp)\n",
        "      print(\"pp\")\n",
        "      print(train_dataset.shape)\n",
        "      np.random.shuffle(train_dataset)\n",
        "      tree = DecisionTree()\n",
        "    # Construct a tree using training set\n",
        "      learnt_tree=tree.learn( train_dataset )\n",
        "\n",
        "    # Classify the test set using the tree we just constructed\n",
        "      results_test = []\n",
        "      for instance in test_dataset:\n",
        "        result = tree.classify( instance[:-1], learnt_tree )\n",
        "        results_test.append( result == instance[-1])\n",
        "      \n",
        "      results_train = []\n",
        "      for instance in train_dataset:\n",
        "        result = tree.classify( instance[:-1], learnt_tree )\n",
        "        results_train.append( result == instance[-1])\n",
        "\n",
        "\n",
        "    # Accuracy\n",
        "      accuracy_train = float(results_train.count(True))/float(len(results_train))\n",
        "      print (\" Train accuracy at fold \"+ str(k+1)+ \": \" +str(accuracy_train))       \n",
        "      Acc_train.append(accuracy_train)\n",
        "    \n",
        "      accuracy_test = float(results_test.count(True))/float(len(results_test))\n",
        "      print (\" Test accuracy at fold \"+ str(k+1)+ \": \" +str(accuracy_test))       \n",
        "      Acc_test.append(accuracy_test)\n",
        "      print(\"------END OF FOLD \"+str(k+1)+\" -----\")\n",
        "    \n",
        "    return Acc_test, Acc_train\n",
        "    \n",
        "    '''\n",
        "    # Writing results to a file (DO NOT CHANGE)\n",
        "    f = open(myname+\"result.txt\", \"w\")\n",
        "    f.write(\"accuracy: %.4f\" % accuracy)\n",
        "    f.close()\n",
        "    '''\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Acc_test, Acc_train = run_decision_tree()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records: 4898\n",
            "Total positive samples: 1060\n",
            "Total negative samples: 3838\n",
            "Pos_data in training set: (954, 12)\n",
            "Neg_data in training set: (3455, 12)\n",
            " Train accuracy at fold 1: 0.8437106125535669\n",
            " Test accuracy at fold 1: 0.8190045248868778\n",
            "------END OF FOLD 1 -----\n",
            " Train accuracy at fold 2: 0.8391731787244769\n",
            " Test accuracy at fold 2: 0.8190045248868778\n",
            "------END OF FOLD 2 -----\n",
            " Train accuracy at fold 3: 0.8553062767834636\n",
            " Test accuracy at fold 3: 0.8076923076923077\n",
            "------END OF FOLD 3 -----\n",
            " Train accuracy at fold 4: 0.8459793294681119\n",
            " Test accuracy at fold 4: 0.8190045248868778\n",
            "------END OF FOLD 4 -----\n",
            " Train accuracy at fold 5: 0.8432459677419355\n",
            " Test accuracy at fold 5: 0.8344671201814059\n",
            "------END OF FOLD 5 -----\n",
            " Train accuracy at fold 6: 0.8384983623078861\n",
            " Test accuracy at fold 6: 0.8068181818181818\n",
            "------END OF FOLD 6 -----\n",
            " Train accuracy at fold 7: 0.8480725623582767\n",
            " Test accuracy at fold 7: 0.7954545454545454\n",
            "------END OF FOLD 7 -----\n",
            " Train accuracy at fold 8: 0.8478206097253717\n",
            " Test accuracy at fold 8: 0.8113636363636364\n",
            "------END OF FOLD 8 -----\n",
            " Train accuracy at fold 9: 0.8465608465608465\n",
            " Test accuracy at fold 9: 0.8136363636363636\n",
            "------END OF FOLD 9 -----\n",
            " Train accuracy at fold 10: 0.8458049886621315\n",
            " Test accuracy at fold 10: 0.8159090909090909\n",
            "------END OF FOLD 10 -----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuMtw3gcC8FN",
        "outputId": "e086fdf2-b9b0-4c07-9893-7204b620125a"
      },
      "source": [
        "print(\" Final Train accuracy after 10 folds: \"+ str(np.average(np.array(Acc_train)))+ \"+/- \"+str(np.std(np.array(Acc_train))))\n",
        "print(\" Final Test accuracy after 10 folds: \"+ str(np.average(np.array(Acc_test)))+ \" +/- \"+str(np.std(np.array(Acc_test))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Final Train accuracy after 10 folds: 0.8454172734886066+/- 0.004548409294559074\n",
            " Final Test accuracy after 10 folds: 0.8142354820716164 +/- 0.009692837304730398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPdXYBJ6HLF-"
      },
      "source": [
        "## Univariate Three-way Split Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sueeVryKHQeu"
      },
      "source": [
        "class NodeMulti:\n",
        "  def __init__(self):\n",
        "    self.left_child=None\n",
        "    self.right_child=None\n",
        "    self.middle_child= None\n",
        "    self.impurity_value=None\n",
        "    self.feature_value=None\n",
        "    self.threshold = []\n",
        "    self.info_gain = None\n",
        "    self.label=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHmgWBmRH0Pa"
      },
      "source": [
        "Method to calculate Information gain. Uses Gini and Entropy methods from above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PewCF_nYHRde"
      },
      "source": [
        "def Info_Gain_multi(left_data, right_data, middle_data, data):\n",
        "\n",
        "  n_left = left_data.shape[0]\n",
        "  n_right = right_data.shape[0]\n",
        "  n_middle = middle_data.shape[0]\n",
        "  total_ = data.shape[0]\n",
        "\n",
        "  # Use Gini Index\n",
        "  #impurity_value = ((n_left/total_) * gini(left_data))+ ((n_right/total_) * gini(right_data)) +((n_middle/total_) * gini(middle_data))\n",
        "  #info_gain = gini(data) - impurity_value\n",
        "\n",
        "  # Use Entropy Value\n",
        "  impurity_value = ((n_left/total_) * entropy(left_data))+ ((n_right/total_) * entropy(right_data)) +((n_middle/total_) * entropy(middle_data))\n",
        "  info_gain = entropy(data) - impurity_value\n",
        "\n",
        "  return info_gain, impurity_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyYNkt16RAj3"
      },
      "source": [
        "Method to split a node three way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFNjDvFVHpLN"
      },
      "source": [
        "def node_multisplit(data):\n",
        "  max_feature={'threshold':[],'impurity_value':None, 'feature_index':None, 'info_gain': None,'left_data':None, 'right_data':None, 'middle_data':None}\n",
        "  max_ig = -100000000\n",
        "  for i in range(data.shape[1]-1): # loop over each feature\n",
        "    unique_values = np.unique(data[:,i])\n",
        "    for j in range(5,len(unique_values)-5):   # loop over unique value of each feature\n",
        "      # Splits data into three parts\n",
        "      left_data = np.array([row for row in data if row[i]<= unique_values[j-5]]) # Change the integer value to have more wider range using which we split the data. Change the value accordingly in the for loop.\n",
        "      middle_data = np.array([row for row in data if (row[i]< unique_values[j+5] and row[i]> unique_values[j-5])])\n",
        "      right_data = np.array([row for row in data if (row[i]>=unique_values[j+5])])\n",
        "\n",
        "      if left_data.shape[0] > 0 and right_data.shape[0] > 0 and middle_data.shape[0] > 0:\n",
        "        info_gain, impurity_value = Info_Gain_multi(left_data[:,-1], right_data[:,-1], middle_data[:,-1], data[:,-1])\n",
        "        if info_gain > max_ig:\n",
        "          max_ig = info_gain\n",
        "          max_feature['threshold'] = [unique_values[j-5], unique_values[j+5]]\n",
        "          max_feature['impurity_value'] = impurity_value\n",
        "          max_feature['feature_index'] = i\n",
        "          max_feature['info_gain'] = info_gain\n",
        "          max_feature['left_data'] = left_data\n",
        "          max_feature['right_data'] = right_data\n",
        "          max_feature['middle_data'] = middle_data\n",
        "  \n",
        "  return max_feature['feature_index'], max_feature['left_data'], max_feature['right_data'], max_feature['middle_data'],max_feature['threshold'], max_feature['impurity_value'], max_feature['info_gain']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rucaOx7XRMmv"
      },
      "source": [
        "Build tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfA9qlXoQgjx"
      },
      "source": [
        "def tree_multi(data, current_depth):\n",
        "  \n",
        "\n",
        "  max_depth = 3\n",
        "  min_sample = 30\n",
        "  \n",
        "  if(current_depth <= max_depth and data.shape[0] >= min_sample):\n",
        "    feature,left_data,right_data, middle_data, threshold, impurity_value, info_gain = node_multisplit(data) \n",
        "    node=NodeMulti()\n",
        "    node.feature_value = feature\n",
        "    node.threshold= threshold\n",
        "    node.impurity_value = impurity_value\n",
        "    node.info_gain = info_gain\n",
        "    node.left_child = tree_multi(left_data, current_depth+1)\n",
        "    node.middle_child = tree_multi(middle_data, current_depth+1)\n",
        "    node.right_child = tree_multi(right_data, current_depth+1)\n",
        "    return node\n",
        "\n",
        "# Base Case\n",
        "  node=NodeMulti()\n",
        "  pos = data[:,-1].tolist().count(1)\n",
        "  neg = data[:,-1].tolist().count(0)\n",
        "  if pos >= neg:\n",
        "    node.label= int(1)\n",
        "  else:\n",
        "    node.label = int(0)\n",
        "  \n",
        "  return node\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73SjIeX8Qrcs"
      },
      "source": [
        "# FoML Assign 1 Code Skeleton\n",
        "# Please use this outline to implement your decision tree. You can add any code around this.\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Enter You Name Here\n",
        "myname = \"Aveen-Dayal\" # or \"Amar-Akbar-Antony\"\n",
        "\n",
        "# Implement your decision tree below\n",
        "class DecisionTree():\n",
        "    tree = {}\n",
        "\n",
        "    def learn(self, training_dataset):\n",
        "        # implement this function\n",
        "\n",
        "        self.tree = tree_multi(training_dataset,0)\n",
        "        return self.tree\n",
        "\n",
        "    # implement this function\n",
        "    def classify(self, test_instance,tree):\n",
        "        #result = 0 # baseline: always classifies as 0\n",
        "        if tree.label != None:  \n",
        "          result = tree.label\n",
        "          return result\n",
        "        else:\n",
        "          instance_val = test_instance[tree.feature_value]\n",
        "          if instance_val <= tree.threshold[0]:\n",
        "            return self.classify(test_instance, tree.left_child)\n",
        "          elif instance_val > tree.threshold[0] and instance_val < tree.threshold[1]:\n",
        "            return self.classify(test_instance, tree.middle_child)\n",
        "          else:\n",
        "            return self.classify(test_instance, tree.right_child)\n",
        "\n",
        "def run_decision_tree():\n",
        "\n",
        "    # Load data set\n",
        "    with open(\"/gdrive/MyDrive/foml datasets/wine-dataset.csv\") as f:\n",
        "        next(f, None)\n",
        "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
        "    print (\"Number of records: \"+ str(len(data)))\n",
        "\n",
        "    # Split training/test sets\n",
        "    # You need to modify the following code for cross validation.\n",
        "\n",
        "    data=np.array(data).astype(np.float)\n",
        "    np.random.shuffle(data)\n",
        "    training_set,test_set = stratified_train_test_split(data)\n",
        "\n",
        "\n",
        "    n_folds = 10\n",
        "    fold_data = stratified_K_fold(training_set, n_folds)\n",
        "\n",
        "    Acc_train=[]\n",
        "    Acc_test=[]\n",
        "\n",
        "    for k in range(n_folds):\n",
        "      test_dataset = fold_data[k]\n",
        "      temp=[]\n",
        "      for j in range(n_folds):\n",
        "        if j != k:\n",
        "          temp.append(fold_data[j])\n",
        "      train_dataset = np.vstack(temp)\n",
        "      np.random.shuffle(train_dataset)\n",
        "      tree = DecisionTree()\n",
        "    # Construct a tree using training set\n",
        "      learnt_tree=tree.learn( train_dataset )\n",
        "\n",
        "    # Classify the test set using the tree we just constructed\n",
        "      results_test = []\n",
        "      for instance in test_dataset:\n",
        "        result = tree.classify( instance[:-1], learnt_tree )\n",
        "        results_test.append( result == instance[-1])\n",
        "      \n",
        "      results_train = []\n",
        "      for instance in train_dataset:\n",
        "        result = tree.classify( instance[:-1], learnt_tree )\n",
        "        results_train.append( result == instance[-1])\n",
        "\n",
        "\n",
        "    # Accuracy\n",
        "      accuracy_train = float(results_train.count(True))/float(len(results_train))\n",
        "      print (\" Train accuracy at fold \"+ str(k+1)+ \": \" +str(accuracy_train))       \n",
        "      Acc_train.append(accuracy_train)\n",
        "\n",
        "      accuracy_test = float(results_test.count(True))/float(len(results_test))\n",
        "      print (\" Test accuracy at fold \"+ str(k+1)+ \": \" +str(accuracy_test))       \n",
        "      Acc_test.append(accuracy_test)\n",
        "      print(\"------END OF FOLD \"+str(k+1)+\" -----\")\n",
        "    \n",
        "    return Acc_test, Acc_train\n",
        "    \n",
        "    '''\n",
        "    # Writing results to a file (DO NOT CHANGE)\n",
        "    f = open(myname+\"result.txt\", \"w\")\n",
        "    f.write(\"accuracy: %.4f\" % accuracy)\n",
        "    f.close()\n",
        "    '''\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Acc_test, Acc_train = run_decision_tree()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcs4-E9sVlLe"
      },
      "source": [
        "print(\" Final Train accuracy after 10 folds: \"+ str(np.average(np.array(Acc_train)))+ \"+/- \"+str(np.std(np.array(Acc_train))))\n",
        "print(\" Final Test accuracy after 10 folds: \"+ str(np.average(np.array(Acc_test)))+ \" +/- \"+str(np.std(np.array(Acc_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNW_4A9Nq03Q"
      },
      "source": [
        "## Multi Variate Binary Split Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xsieNqisKZU"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/gdrive/MyDrive/foml datasets/wine-dataset.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G63WOY7sQFa"
      },
      "source": [
        "Find correlation between features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtTWlewVsM1N"
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI6cXWDAsSur"
      },
      "source": [
        "As explained in the report we select top 4 least correlated features for each feature and use that for findinf best split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy37SIGAsO8t"
      },
      "source": [
        "feature_corr={'0':[1,4,9,10],'1':[0,7,8,9], '2':[1,4,6,7],'3':[1,4,8,9],'4':[0,1,8,9],'5':[0,1,8,9],'6':[0,1,8,9],'7':[1,2,8,9],'8':[1,4,5,6,],'9':[0,3,4,10],'10':[1,2,8,9]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s521ugWlsdrY"
      },
      "source": [
        "Node class for mutlivariate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMudLvMLqzrA"
      },
      "source": [
        "class NodeMultivar:\n",
        "  def __init__(self):\n",
        "    self.left_child=None\n",
        "    self.right_child=None\n",
        "    self.impurity_value=None\n",
        "    self.feature_value=[]\n",
        "    self.threshold = []\n",
        "    self.info_gain = None\n",
        "    self.label=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCM2gqqdrYjV"
      },
      "source": [
        "Method to split node using 2 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5wDerPsq_J6"
      },
      "source": [
        "def multi_node_split(data):\n",
        "  max_feature={'threshold':None,'impurity_value':None, 'feature_index':None, 'info_gain': None,'left_data':None, 'right_data':None}\n",
        "  max_ig = -100000000\n",
        "  for i in range(data.shape[1]-1):\n",
        "    for k in feature_corr[str(i)][:2]: \n",
        "      unique_values1 = np.unique(data[:,i])\n",
        "      unique_values2 = np.unique(data[:,k])\n",
        "      for m in range(len(unique_values1)):\n",
        "        for j in range(len(unique_values2)):\n",
        "          mid1= (unique_values1[m])\n",
        "          mid2 = (unique_values2[j])\n",
        "          temp1=[]\n",
        "          temp2=[]\n",
        "          for row in data:\n",
        "            if row[i]==mid1 and row[k]==mid2:\n",
        "              temp1.append(row)\n",
        "            elif row[i]!=mid1 or row[k]!=mid2:\n",
        "            temp2.append(row)\n",
        "          left_data = np.array(temp1)\n",
        "          right_data = np.array(temp2)\n",
        "          if left_data.shape[0] > 0 and right_data.shape[0] > 0:\n",
        "            info_gain, impurity_value = Info_Gain(left_data[:,-1], right_data[:,-1], data[:,-1])\n",
        "            if info_gain > max_ig:\n",
        "              max_ig = info_gain\n",
        "              max_feature['threshold'] = [mid1, mid2]\n",
        "              max_feature['impurity_value'] = impurity_value\n",
        "              max_feature['feature_index'] = [i,k]\n",
        "              max_feature['info_gain'] = info_gain\n",
        "              max_feature['left_data'] = left_data\n",
        "              max_feature['right_data'] = right_data\n",
        "\n",
        "  return max_feature['feature_index'], max_feature['left_data'], max_feature['right_data'], max_feature['threshold'], max_feature['impurity_value'], max_feature['info_gain']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-oaqd_bsCLC"
      },
      "source": [
        "Build Multi variate tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXmDJviJrKGz"
      },
      "source": [
        "def tree_multi_var(data, current_depth):\n",
        "  \n",
        "\n",
        "  max_depth = 4\n",
        "  min_sample = 30\n",
        "\n",
        "  if(current_depth <= max_depth and data.shape[0] >= min_sample):\n",
        "    feature,left_data,right_data, threshold, impurity_value, info_gain = multi_node_split(data)\n",
        "    node=NodeMultivar()\n",
        "    node.feature_value = feature\n",
        "    node.threshold= threshold\n",
        "    node.impurity_value = impurity_value\n",
        "    node.info_gain = info_gain\n",
        "    node.left_child = tree_multi_var(left_data, current_depth+1)\n",
        "    node.right_child = tree_multi_var(right_data, current_depth+1)\n",
        "    return node\n",
        "\n",
        "# Base Case\n",
        "  node=NodeMultivar()\n",
        "  pos = data[:,-1].tolist().count(1)\n",
        "  neg = data[:,-1].tolist().count(0)\n",
        "  if pos >= neg:\n",
        "    node.label= int(1)\n",
        "  else:\n",
        "    node.label = int(0)\n",
        "  \n",
        "  return node\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NFTE7J-ry1y"
      },
      "source": [
        "# FoML Assign 1 Code Skeleton\n",
        "# Please use this outline to implement your decision tree. You can add any code around this.\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Enter You Name Here\n",
        "myname = \"Aveen-Dayal\" # or \"Amar-Akbar-Antony\"\n",
        "\n",
        "# Implement your decision tree below\n",
        "class DecisionTree():\n",
        "    tree = {}\n",
        "\n",
        "    def learn(self, training_dataset):\n",
        "        # implement this function\n",
        "\n",
        "        self.tree = tree_multi_var(training_dataset,0)\n",
        "        return self.tree\n",
        "\n",
        "    # implement this function\n",
        "    def classify(self, test_instance,tree):\n",
        "        #result = 0 # baseline: always classifies as 0\n",
        "        if tree.label != None:  \n",
        "          result = tree.label\n",
        "          return result\n",
        "        else:\n",
        "          instance_val1 = test_instance[tree.feature_value[0]]\n",
        "          instance_val2 = test_instance[tree.feature_value[1]]\n",
        "                    \n",
        "          if instance_val1 == tree.threshold[0] and instance_val2 == tree.threshold[1]:\n",
        "            return self.classify(test_instance, tree.left_child)\n",
        "          elif instance_val1 != tree.threshold[0] or instance_val2 != tree.threshold[1]:\n",
        "            return self.classify(test_instance, tree.right_child)\n",
        "          \n",
        "\n",
        "def run_decision_tree():\n",
        "\n",
        "    # Load data set\n",
        "    with open(\"/gdrive/MyDrive/foml datasets/wine-dataset.csv\") as f:\n",
        "        next(f, None)\n",
        "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
        "    print (\"Number of records: \"+ str(len(data)))\n",
        "\n",
        "    # Split training/test sets\n",
        "    # You need to modify the following code for cross validation.\n",
        "\n",
        "    data=np.array(data).astype(np.float)\n",
        "    np.random.shuffle(data)\n",
        "    training_set,test_set = stratified_train_test_split(data)\n",
        "\n",
        "    n_folds = 10\n",
        "    fold_data = stratified_K_fold(training_set, n_folds)\n",
        "\n",
        "    Acc_test=[]\n",
        "    Acc_train=[]\n",
        "\n",
        "    for k in range(n_folds):\n",
        "      test_dataset = fold_data[k]\n",
        "      temp=[]\n",
        "      for j in range(n_folds):\n",
        "        if j != k:\n",
        "          temp.append(fold_data[j])\n",
        "      train_dataset = np.vstack(temp)\n",
        "      np.random.shuffle(train_dataset)\n",
        "      tree = DecisionTree()\n",
        "    # Construct a tree using training set\n",
        "      learnt_tree=tree.learn( train_dataset )\n",
        "\n",
        "    # Classify the test set using the tree we just constructed\n",
        "      results_test = []\n",
        "      for instance in test_dataset:\n",
        "        result = tree.classify( instance[:-1], learnt_tree )\n",
        "        results_test.append( result == instance[-1])\n",
        "      \n",
        "      results_train = []\n",
        "      for instance in train_dataset:\n",
        "        result = tree.classify( instance[:-1], learnt_tree )\n",
        "        results_train.append( result == instance[-1])\n",
        "\n",
        "\n",
        "    # Accuracy\n",
        "      accuracy_train = float(results_train.count(True))/float(len(results_train))\n",
        "      print (\" Train accuracy at fold \"+ str(k+1)+ \": \" +str(accuracy_train))       \n",
        "      Acc_train.append(accuracy_train)\n",
        "\n",
        "      accuracy_test = float(results_test.count(True))/float(len(results_test))\n",
        "      print (\" Test accuracy at fold \"+ str(k+1)+ \": \" +str(accuracy_test))       \n",
        "      Acc_test.append(accuracy_test)\n",
        "      print(\"------END OF FOLD \"+str(k+1)+\" -----\")\n",
        "    \n",
        "    return Acc_test, Acc_train\n",
        "    \n",
        "    '''\n",
        "    # Writing results to a file (DO NOT CHANGE)\n",
        "    f = open(myname+\"result.txt\", \"w\")\n",
        "    f.write(\"accuracy: %.4f\" % accuracy)\n",
        "    f.close()\n",
        "    '''\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Acc_test, Acc_train = run_decision_tree()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}